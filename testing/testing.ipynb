{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580a2652-8eb4-4ab4-9c86-2a6f6ae631bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dxgcl0g0iUw",
    "outputId": "9844d40f-7c5a-49ad-ac42-a9e6379bcb6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 21 14:41:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045d4ac6-b7a5-4dca-a9a3-06e5c5da1087",
   "metadata": {
    "id": "Gy6_XE7M2hOo"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torchinfo rouge_score git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59481c37-6984-4434-b17e-5e46ec73dc78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiKi_g-93-nG",
    "outputId": "0ff88440-7a58-45cb-eb72-dc8b21c97243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5abec24-acc7-45fd-a9d7-e1edd2044333",
   "metadata": {
    "id": "kUyFtE2WbINv"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42d1c9-cda7-4d2b-8461-2f52c5c32b41",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184c82a-8b59-4147-976a-5a94b1e8f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already tokenized dataset\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "DATA_PATH = \"/gdrive/MyDrive/final-project/post-refactor/data/\"\n",
    "TOKENIZED_DATASET_PATH = os.path.join(DATA_PATH, \"tokenized_bigbird_dataset\")\n",
    "\n",
    "dataset = load_from_disk(TOKENIZED_DATASET_PATH)\n",
    "\n",
    "# split dataset into test and train\n",
    "dataset = dataset.train_test_split(test_size=0.10, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca27760-f6d1-4753-99a4-5957fce5a2c9",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ce6cf-6474-4c6e-ac6d-afbb1a042995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration\n",
    "from torchinfo import summary\n",
    "\n",
    "FINETUNE_MODEL_PATH = os.path.join(DATA_PATH, \"BigBirdModelFineTune/\", \"final/\")\n",
    "\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
    "    FINETUNE_MODEL_PATH,\n",
    "    block_size=16,\n",
    "    num_random_blocks=3,\n",
    "    attention_type=\"block_sparse\",\n",
    "    use_cache=False) # required for fp16\n",
    "model.gradient_checkpointing_enable()\n",
    "summary(model, dtypes=[\"torch.IntTensor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5e1f8-6c38-40ae-8c8e-a9bcc67a258f",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e1819-817e-430c-8c31-b2d62bf3b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
    "valid_sample = dataset[\"valid\"][\"input_ids\"][0]\n",
    "\n",
    "# print summary\n",
    "tokenizer.decode(valid_sample, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf04d-381b-40b0-9fda-13da204f9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print claim\n",
    "tokenizer.decode(dataset[\"valid\"][\"decoder_input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a5383-28da-4c19-8825-845ca826f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "valid_sample = torch.tensor(valid_sample)\n",
    "inputs = tokenizer([tokenizer.decode(valid_sample, skip_special_tokens=True)], \n",
    "                   max_length=2048, \n",
    "                   return_tensors=\"pt\", \n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b852e39-d7c8-403b-b955-96b8b2723680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    #no_repeat_ngram_size=2,\n",
    "    repetition_penalty=0.5,\n",
    "    #num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "ROUGE = load_metric('rouge')\n",
    "SACREBLEU = load_metric('sacrebleu')\n",
    "BLEURT = load_metric('bleurt', 'bleurt-large-512')\n",
    "SARI = load_metric('sari')\n",
    "\n",
    "def compute_metrics(input, reference, predicted): \n",
    "  d_input = tokenizer.batch_decode(input, skip_special_tokens=True)\n",
    "  d_pred = tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
    "  # Replace -100 in the labels to actual padding\n",
    "  reference = torch.where(reference != -100, reference, tokenizer.pad_token_id)\n",
    "  d_label = tokenizer.batch_decode(reference, skip_special_tokens=True)\n",
    "\n",
    "  rouge_scores = ROUGE.compute(references=reference, predictions=predicted)\n",
    "  rouge_scores = { k: v.mid.fmeasure * 100 for k, v in rouge_scores.items() }\n",
    "\n",
    "  sacrebleu_score = SACREBLEU.compute(predictions=d_pred, references=[d_label])\n",
    "  sacrebleu_score = sacrebleu_score[\"score\"]\n",
    "\n",
    "  bleurt_score = BLEURT.compute(predictions=d_pred, references=d_label)\n",
    "  bleurt_score = bleurt_score[\"scores\"][0] * 100\n",
    "\n",
    "  sari_score = SARI.compute(predictions=d_pred, sources=d_input, references=[d_label])\n",
    "  sari_score = sari_score[\"sari\"]\n",
    "  \n",
    "  return {\n",
    "      \"sacrebleu\": sacrebleu_score, \n",
    "      **rouge_scores, \n",
    "      \"bleurt\": bleurt_score,\n",
    "      \"sari\": sari_score\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
