{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dxgcl0g0iUw",
        "outputId": "7bb6e094-6bd3-4a21-f60b-a2b10c1fac9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 19 15:07:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gy6_XE7M2hOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fce8fee-4405-4a9c-cebe-e6a477220ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.0 MB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 70.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 64.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 11.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKi_g-93-nG",
        "outputId": "7ed32191-1d68-4854-c6ce-1b2b9a768692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kUyFtE2WbINv"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgX8uQt9oCy",
        "tags": []
      },
      "source": [
        "# Dataset preparation\n",
        "\n",
        "We will tokenize the whole data by making use of the `datasets` library, which works seamlessly with the huggingface library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T4HuiUzWJ9Bw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "DATA_PATH = \"/gdrive/MyDrive/final-project/post-refactor/data/\"\n",
        "DATASET_CSV_PATH = os.path.join(DATA_PATH, \"balanced_summaries_claims.zip\")\n",
        "\n",
        "# set maximum csv size to avoid pandas ram issues\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "# load into memory for analysis\n",
        "df = pd.read_csv(DATASET_CSV_PATH, engine=\"python\").set_index(\"patentnumber\")[[\"summary\", \"claim\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDiYSQrqf5oy"
      },
      "source": [
        "## Dataset tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, we won't be able to use the whole dataset. That is because BigBird-Pegasus requires a huge amount of memory in order to be finetuned.\n",
        "\n",
        "To avoid incurring into out-of-memory errors from CUDA we will only use a subset of the available dataset, by taking only those documents whose claim length is shorter than 500 tokens."
      ],
      "metadata": {
        "id": "gk-pFOEzbKUl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PPlNMJltf5o1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "summary_len = df.summary.apply(lambda r: len(r.split()))\n",
        "claim_len = df.claim.apply(lambda r: len(r.split()))\n",
        "\n",
        "MAX_SUMMARY_LEN = None #@param\n",
        "MAX_CLAIM_LEN = 500 #@param\n",
        "\n",
        "subset_clause = summary_len >= 0\n",
        "if MAX_SUMMARY_LEN is not None:\n",
        "  subset_clause = subset_clause & (summary_len < MAX_SUMMARY_LEN)\n",
        "if MAX_CLAIM_LEN is not None:\n",
        "  subset_clause = subset_clause & (claim_len < MAX_CLAIM_LEN)\n",
        "\n",
        "subset_df = df[subset_clause]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3DzDN88-Dc9",
        "outputId": "71de5b5c-62b9-4830-8235-b96a1bc435dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Tokenize data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
        "TOKENIZED_DATASET_PATH = os.path.join(DATA_PATH, \"tokenized_bigbird_dataset\")\n",
        "\n",
        "# let's check if we can load the dataset from disk first.\n",
        "# this will save us the burden of loading the tokenizer\n",
        "# and tokenizing all the data we need\n",
        "if os.path.exists(TOKENIZED_DATASET_PATH):\n",
        "  dataset = load_from_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset loaded\")\n",
        "else:\n",
        "  from datasets import Dataset\n",
        "  dataset = Dataset.from_pandas(subset_df)\n",
        "\n",
        "  # first let's rename data in the way the model expect\n",
        "  dataset = dataset.rename_column(\"summary\", \"input_ids\") \\\n",
        "    .rename_column(\"claim\", \"decoder_input_ids\")\n",
        "  # even though we carefully preprocessed data some descriptions are still empty.\n",
        "  # we will filter them out\n",
        "  dataset = dataset.filter(lambda r: r[\"input_ids\"] is not None)\n",
        "\n",
        "  def encoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize the summary into input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_SUMMARY_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_SUMMARY_LEN\n",
        "\n",
        "    return tokenizer(row[\"input_ids\"], **kwargs)\n",
        "\n",
        "  # tokenize the summaries\n",
        "  dataset = dataset.map(encoder_tokenize_function, batched=True)\n",
        "\n",
        "  def decoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize claim into the expected output from the decoder \n",
        "    (decoder_input_ids and decoder_attention_mask)\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_CLAIM_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_CLAIM_LEN\n",
        "\n",
        "    tokenized = tokenizer(row[\"decoder_input_ids\"], **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"decoder_input_ids\": tokenized[\"input_ids\"],\n",
        "        \"decoder_attention_mask\": tokenized[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "  # tokenize the claim\n",
        "  dataset = dataset.map(decoder_tokenize_function, batched=True)\n",
        "\n",
        "  def compute_labels(row):\n",
        "    \"\"\"\n",
        "    Compute labels based on decoder_input_ids where padding token is represented as -100\n",
        "    \"\"\"\n",
        "    labels = row[\"decoder_input_ids\"]\n",
        "    labels = [-100 if t == 0 else t for t in labels]\n",
        "    return {\"labels\" : labels}\n",
        "  \n",
        "  dataset = dataset.map(compute_labels, batched=True)\n",
        "\n",
        "  # export the dataset to disk for future loading\n",
        "  dataset.save_to_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset computed and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYakYEpgao7F"
      },
      "source": [
        "# Dataset splits\n",
        "\n",
        "We will divide the overall dataset into the usual splits: training and testing respectively $90\\%$ and $10\\%$ of the overall data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7mPyx-eCbLD9"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.10, seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbS0JPhCcrE9",
        "outputId": "26511a06-62e6-4ad1-d521-069ffe6b916f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1249 samples\n",
            "Test: 139 samples\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Test: {len(dataset['test'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIggHc80KgIm"
      },
      "source": [
        "# Neural model\n",
        "\n",
        "We will use Google's BigbirdPegasus (*BP*) model, by making use of huggingface APIs.\n",
        "[Bigbird](https://arxiv.org/pdf/2007.14062v2.pdf) represents the BERT variant in which attention is computed.\n",
        "\n",
        "\n",
        "The model is made public with an already pretrained version on a patent dataset, [BIGPATENT](https://arxiv.org/pdf/1906.03741v1.pdf) different from the one we're using.\n",
        "The dataset consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.\n",
        "\n",
        "While general abstractive summarization is a different task than the one we're interested in, it's fairly easy to suppose that an indipendent claim could be interpreted as the summarization of the patent from a particular perspective.\n",
        "\n",
        "This model, which represents the current SOTA in the abstractive summarization field, is an incredible resource in the problem we're trying to solve.\n",
        "\n",
        "We will initially try to generate claims without any work on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ul_ktbMf4a",
        "outputId": "5e380b53-2dc1-4d7e-87fc-cca64d9e1b92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                                                      Param #\n",
              "====================================================================================================\n",
              "BigBirdPegasusForConditionalGeneration                                      --\n",
              "├─BigBirdPegasusModel: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                       98,409,472\n",
              "│    └─BigBirdPegasusEncoder: 2-2                                           --\n",
              "│    │    └─Embedding: 3-1                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-2                   4,194,304\n",
              "│    │    └─ModuleList: 3-3                                                 201,474,048\n",
              "│    │    └─LayerNorm: 3-4                                                  2,048\n",
              "│    └─BigBirdPegasusDecoder: 2-3                                           --\n",
              "│    │    └─Embedding: 3-5                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-6                   4,194,304\n",
              "│    │    └─ModuleList: 3-7                                                 268,615,680\n",
              "│    │    └─LayerNorm: 3-8                                                  2,048\n",
              "├─Linear: 1-2                                                               98,409,472\n",
              "====================================================================================================\n",
              "Total params: 675,301,376\n",
              "Trainable params: 675,301,376\n",
              "Non-trainable params: 0\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration\n",
        "from torchinfo import summary\n",
        "\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
        "    \"google/bigbird-pegasus-large-bigpatent\",\n",
        "    block_size=16,\n",
        "    num_random_blocks=3,\n",
        "    attention_type=\"block_sparse\") # required for fp16\n",
        "model.gradient_checkpointing_enable()\n",
        "summary(model, dtypes=[\"torch.IntTensor\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GqLy7-vl5wnq"
      },
      "outputs": [],
      "source": [
        "FINETUNE_MODEL_PATH = os.path.join(DATA_PATH, \"BigBirdModelFineTune/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ue7Jzvknuskl"
      },
      "outputs": [],
      "source": [
        "from transformers import  Seq2SeqTrainingArguments,  Seq2SeqTrainer\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "training_args =  Seq2SeqTrainingArguments(\n",
        "    output_dir=FINETUNE_MODEL_PATH,\n",
        "    overwrite_output_dir=True, # used to keep training\n",
        "    gradient_accumulation_steps=1, # lower memory usage: perform backprop every 2 steps\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=2, # save at most two checkpoints, delete the older ones\n",
        "    fp16=True, # faster and lighter on memory but possibly less precise on convergence\n",
        "    predict_with_generate=True,\n",
        "    gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5gL4AH3yggpT",
        "outputId": "5de16424-47c2-46a6-bb9b-2c80f45bf3c1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n",
            "The following columns in the training set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1249\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 625\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='503' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [503/625 36:24 < 08:52, 0.23 it/s, Epoch 0.80/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>10.845300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.276600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.464500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.074600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-250\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-250/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-250/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-500\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-500/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 45:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>10.845300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.276600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.464500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.074600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.108800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=1.6715751320838927, metrics={'train_runtime': 2706.6577, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.231, 'total_flos': 1.4429729247461376e+16, 'train_loss': 1.6715751320838927, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, \n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(os.path.join(FINETUNE_MODEL_PATH, \"final/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7KqYaU-qJQ5",
        "outputId": "50a2cfc0-e8a8-4d57-d52e-a02c986526f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/pytorch_model.bin\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training_bigbird.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}