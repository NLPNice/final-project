{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dxgcl0g0iUw",
        "outputId": "9844d40f-7c5a-49ad-ac42-a9e6379bcb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 21 14:41:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gy6_XE7M2hOo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torchinfo rouge_score git+https://github.com/google-research/bleurt.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y3R2Y9djqYA",
        "outputId": "96a9824b-845e-4383-e8b3-b0cdb1d760d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKi_g-93-nG",
        "outputId": "0ff88440-7a58-45cb-eb72-dc8b21c97243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kUyFtE2WbINv"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgX8uQt9oCy"
      },
      "source": [
        "# Dataset tokenization\n",
        "\n",
        "We will tokenize the whole data by making use of the `datasets` library, which works seamlessly with the huggingface library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TMDMMsUOPjkw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "import os\n",
        "\n",
        "GDRIVE_DATASET_PATH = \"/gdrive/MyDrive/university/tokenized_dataset\"\n",
        "SUMMARIES_CLAIMS_CSV_PATH = \"/gdrive/MyDrive/university/summaries_claims.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUycMXT-MrXx"
      },
      "source": [
        "## Memory concerns\n",
        "\n",
        "Unfortunately finetuning such a huge model requires a lot of memory in GPU. \n",
        "Google colab limit is $\\approx$ 12 GB which is not enough for the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4HuiUzWJ9Bw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "# load into memory for analysis\n",
        "df = pd.read_csv(SUMMARIES_CLAIMS_CSV_PATH, engine=\"python\")\n",
        "# some descriptions are NaNs so let's drop them\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vre4jAajORYl"
      },
      "source": [
        "Let's check how many samples we would be able to use by using a lower amount of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmHdlj7vNFPb",
        "outputId": "be4c6515-3e6b-45eb-cf47-f80539d77777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11707 summaries have <= 512 tokens (44.97%)\n",
            "25371 claims have <= 512 tokens (97.45%)\n",
            "18100 summaries have <= 1024 tokens (69.52%)\n",
            "25970 claims have <= 1024 tokens (99.75%)\n",
            "23065 summaries have <= 2048 tokens (88.59%)\n",
            "26026 claims have <= 2048 tokens (99.97%)\n",
            "25311 summaries have <= 4096 tokens (97.22%)\n",
            "26033 claims have <= 4096 tokens (99.99%)\n"
          ]
        }
      ],
      "source": [
        "SIZES = [512, 1024, 2048, 4096]\n",
        "\n",
        "summary_tokens = df[\"summaries\"].apply(lambda x: len(x.split(\" \")))\n",
        "claims_tokens = df[\"claims\"].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "for size in SIZES:\n",
        "  ok_summaries = summary_tokens <= size\n",
        "  ok_claims = claims_tokens <= size  \n",
        "  print(f\"{ok_summaries.sum()} summaries have <= {size} tokens ({ok_summaries.sum() / len(summary_tokens) * 100:2.2f}%)\")\n",
        "  print(f\"{ok_claims.sum()} claims have <= {size} tokens ({ok_claims.sum() / len(ok_claims) * 100:2.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kri9SAv6OYeI"
      },
      "source": [
        "We can safely see that by using 512 as maximum length we obtain most of the claims ($97.45\\%$).\n",
        "\n",
        "For the description, however, we can't really go that low or we would lose most of the sample. While using 4096 tokens, which is the maximum length handled by bigbird, would allow us to use all the data in the dataset we don't have adequate resources for such a job.\n",
        "\n",
        "We will therefore make use of only those summaries whose length is $\\leq 2048$, which accounts for $\\approx 88\\%$ of all the available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3DzDN88-Dc9",
        "outputId": "2fcd0a79-0148-49c8-cd94-4ef437efaac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Tokenize data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "SUMMARY_LEN = 2048 #@param {type: \"number\"}\n",
        "CLAIM_LEN = 512 #@param {type: \"number\"}\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
        "\n",
        "\n",
        "# let's check if we can load the dataset from disk first.\n",
        "# this will save us the burden of loading the tokenizer\n",
        "# and tokenizing all the data we need\n",
        "if os.path.exists(GDRIVE_DATASET_PATH):\n",
        "  dataset = load_from_disk(GDRIVE_DATASET_PATH)\n",
        "  print(\"Dataset loaded\")\n",
        "else:\n",
        "  from datasets import Dataset\n",
        "  reduced_df = df[(claims_tokens <= CLAIM_LEN) & (summary_tokens <= SUMMARY_LEN)]\n",
        "  dataset = Dataset.from_pandas(reduced_df)\n",
        "\n",
        "  # first let's rename data in the way the model expect\n",
        "  dataset = dataset.rename_column(\"summaries\", \"input_ids\") \\\n",
        "    .rename_column(\"claims\", \"decoder_input_ids\") \\\n",
        "    .remove_columns(\"patentnumber\") \\\n",
        "    .remove_columns(\"__index_level_0__\")\n",
        "\n",
        "  # even though we carefully preprocessed data some descriptions are still empty.\n",
        "  # we will filter them out\n",
        "  dataset = dataset.filter(lambda r: r[\"input_ids\"] is not None)\n",
        "\n",
        "  def encoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize the summary into input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    return tokenizer(row[\"input_ids\"], max_length=SUMMARY_LEN, padding=\"max_length\", truncation=True)\n",
        "\n",
        "  # tokenize the summaries\n",
        "  dataset = dataset.map(encoder_tokenize_function, batched=True)\n",
        "\n",
        "  def decoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize claim into the expected output from the decoder \n",
        "    (decoder_input_ids and decoder_attention_mask)\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(row[\"decoder_input_ids\"], max_length=CLAIM_LEN, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    return {\n",
        "        \"decoder_input_ids\": tokenized[\"input_ids\"],\n",
        "        \"decoder_attention_mask\": tokenized[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "  # tokenize the claim\n",
        "  dataset = dataset.map(decoder_tokenize_function, batched=True)\n",
        "\n",
        "  def compute_labels(row):\n",
        "    \"\"\"\n",
        "    Compute labels based on decoder_input_ids where padding token is represented as -100\n",
        "    \"\"\"\n",
        "    labels = row[\"decoder_input_ids\"]\n",
        "    labels = [-100 if t == 0 else t for t in labels]\n",
        "    return {\"labels\" : labels}\n",
        "  \n",
        "  dataset = dataset.map(compute_labels, batched=True)\n",
        "\n",
        "  # export the dataset to disk for future loading\n",
        "  dataset.save_to_disk(GDRIVE_DATASET_PATH)\n",
        "  print(\"Dataset computed and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct0k9AX4CLmO"
      },
      "source": [
        "We saw that, with the suggested configuration, we can manage to achieve $\\approx 0.55 \\ \\text{it/s}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3foAF12BC91q",
        "outputId": "06c6e5f3-5a83-4985-eddf-e961ec15834b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approximately we would need 10:19:38 to train the whole dataset for 3 epochs\n"
          ]
        }
      ],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "def format(seconds):\n",
        "    h, rem = divmod(seconds, 3600)\n",
        "    m, s = divmod(rem, 60)\n",
        "    return round(h), round(m), round(s)\n",
        "\n",
        "time_per_epoch = len(dataset) * 0.55\n",
        "total_time = time_per_epoch * 3\n",
        "h, m, s = format(total_time)\n",
        "\n",
        "print(f\"Approximately we would need {h}:{m}:{s} to train the whole dataset for 3 epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYakYEpgao7F"
      },
      "source": [
        "# Dataset splits\n",
        "\n",
        "We will divide the overall dataset into the usual splits: training and testing respectively $90\\%$ and $10\\%$ of the overall data.\n",
        "\n",
        "We will further extract $10\\%$ from training data and use it as validation during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mPyx-eCbLD9",
        "outputId": "b6f3471a-b320-47ff-e280-240d2d5a9601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached split indices for dataset at /gdrive/MyDrive/university/tokenized_dataset/cache-b48cb57575d26b9a.arrow and /gdrive/MyDrive/university/tokenized_dataset/cache-b9e15dba1c57d4d3.arrow\n",
            "Loading cached split indices for dataset at /gdrive/MyDrive/university/tokenized_dataset/cache-a07b120e791de5e0.arrow and /gdrive/MyDrive/university/tokenized_dataset/cache-11437103a5c00f24.arrow\n"
          ]
        }
      ],
      "source": [
        "train_test = dataset.train_test_split(test_size=0.15, seed=RANDOM_SEED)\n",
        "test_valid = train_test[\"train\"].train_test_split(test_size=0.1, seed=RANDOM_SEED)\n",
        "train_test[\"train\"] = test_valid[\"train\"]\n",
        "train_test[\"valid\"] = test_valid[\"test\"]\n",
        "dataset = train_test\n",
        "\n",
        "# delete from memory unused values\n",
        "del train_test\n",
        "del test_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbS0JPhCcrE9",
        "outputId": "e0300974-0d7e-411d-c70e-fb7d83d9d5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 17236 samples\n",
            "Test: 3380 samples\n",
            "Valid: 1916 samples\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Test: {len(dataset['test'])} samples\")\n",
        "print(f\"Valid: {len(dataset['valid'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIggHc80KgIm"
      },
      "source": [
        "# Neural model\n",
        "\n",
        "We will use Google's BigbirdPegasus (*BP*) model, by making use of huggingface APIs.\n",
        "[Bigbird](https://arxiv.org/pdf/2007.14062v2.pdf) represents the BERT variant in which attention is computed.\n",
        "\n",
        "\n",
        "The model is made public with an already pretrained version on a patent dataset, [BIGPATENT](https://arxiv.org/pdf/1906.03741v1.pdf) different from the one we're using.\n",
        "The dataset consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.\n",
        "\n",
        "While general abstractive summarization is a different task than the one we're interested in, it's fairly easy to suppose that an indipendent claim could be interpreted as the summarization of the patent from a particular perspective.\n",
        "\n",
        "This model, which represents the current SOTA in the abstractive summarization field, is an incredible resource in the problem we're trying to solve.\n",
        "\n",
        "We will initially try to generate claims without any work on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ul_ktbMf4a",
        "outputId": "f0799ec0-1836-4948-b41c-7522a7e3f1f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                                                      Param #\n",
              "====================================================================================================\n",
              "BigBirdPegasusForConditionalGeneration                                      --\n",
              "├─BigBirdPegasusModel: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                       98,409,472\n",
              "│    └─BigBirdPegasusEncoder: 2-2                                           --\n",
              "│    │    └─Embedding: 3-1                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-2                   4,194,304\n",
              "│    │    └─ModuleList: 3-3                                                 201,474,048\n",
              "│    │    └─LayerNorm: 3-4                                                  2,048\n",
              "│    └─BigBirdPegasusDecoder: 2-3                                           --\n",
              "│    │    └─Embedding: 3-5                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-6                   4,194,304\n",
              "│    │    └─ModuleList: 3-7                                                 268,615,680\n",
              "│    │    └─LayerNorm: 3-8                                                  2,048\n",
              "├─Linear: 1-2                                                               98,409,472\n",
              "====================================================================================================\n",
              "Total params: 675,301,376\n",
              "Trainable params: 675,301,376\n",
              "Non-trainable params: 0\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration\n",
        "from torchinfo import summary\n",
        "\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
        "    \"google/bigbird-pegasus-large-bigpatent\",\n",
        "    block_size=16,\n",
        "    num_random_blocks=3,\n",
        "    attention_type=\"block_sparse\",\n",
        "    use_cache=False) # required for fp16\n",
        "model.gradient_checkpointing_enable()\n",
        "summary(model, dtypes=[\"torch.IntTensor\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jddUKZcCEpb"
      },
      "source": [
        "As we can see the model have a lot of trainable parameters which would take a lot of time to train.\n",
        "\n",
        "The model, however, has already been pretrained on a huge patent dataset and we suppose that the encoder latent dimensions alrady represents a good approximation of what patent documents looks like.\n",
        "\n",
        "We will therefore only finetune the decoder part in order to instruct the network to try and generate claims rather than summarizing the description.\n",
        "Weights composing embeddings, positional embeddings and encoder will be freezed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0EsCQqFeJWe"
      },
      "source": [
        "Huggingface library provide us with several different metrics, in particular we will make use of BLEURT and ROUGE metrics, used to compare the model with other models in literature. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SBpcroGNxrP",
        "outputId": "5d9e1e88-e914-498a-c139-450dcd888f4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                                                      Param #\n",
              "====================================================================================================\n",
              "BigBirdPegasusForConditionalGeneration                                      --\n",
              "├─BigBirdPegasusModel: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                       (98,409,472)\n",
              "│    └─BigBirdPegasusEncoder: 2-2                                           --\n",
              "│    │    └─Embedding: 3-1                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-2                   (4,194,304)\n",
              "│    │    └─ModuleList: 3-3                                                 201,474,048\n",
              "│    │    └─LayerNorm: 3-4                                                  2,048\n",
              "│    └─BigBirdPegasusDecoder: 2-3                                           --\n",
              "│    │    └─Embedding: 3-5                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-6                   (4,194,304)\n",
              "│    │    └─ModuleList: 3-7                                                 268,615,680\n",
              "│    │    └─LayerNorm: 3-8                                                  2,048\n",
              "├─Linear: 1-2                                                               (98,409,472)\n",
              "====================================================================================================\n",
              "Total params: 675,301,376\n",
              "Trainable params: 470,093,824\n",
              "Non-trainable params: 205,207,552\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def freeze_params(model):\n",
        "  \"\"\"\n",
        "  Freeze the parameter of the provided subset of the network\n",
        "  \"\"\"\n",
        "  for par in model.parameters():\n",
        "      par.requires_grad = False\n",
        "\n",
        "def freeze_embeddings_encoder(model):\n",
        "  \"\"\"\n",
        "  Freeze the embeddings and the encoder part of the network.\n",
        "  \"\"\"\n",
        "  freeze_params(model.model.shared)\n",
        "  #freeze_params(model.model.encoder)\n",
        "\n",
        "  for d in [model.model.decoder, model.model.encoder]:\n",
        "    freeze_params(d.embed_positions)\n",
        "    freeze_params(d.embed_tokens)\n",
        "\n",
        "freeze_embeddings_encoder(model)\n",
        "summary(model, dtypes=[\"torch.IntTensor\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342,
          "referenced_widgets": [
            "408ba7074ecd46b584e2f77767665c80",
            "631a4cd6d99a450daa9714ce06e69447",
            "0b50db23543445b28dba32ae911870a0",
            "bf31766f514145579aa234e9d85952a6",
            "d82e3e29a4c849dca30aeaefb37f373d",
            "44d81a184216498cbc18512b911b439b",
            "0a2611c1e1fc4cf0b73f59a606a1ff57",
            "192947130c9f40fd94e795d0be3d53b6",
            "bbc9d12a16524f4095c3413267f6cd1f",
            "d798fc4bccb24367836be34213956acd",
            "0878459731594640b8b5873733edbc2b",
            "fed141bf5b284f23804ed8096af2a7b2",
            "eb125595968344bf9232d03264b40bd2",
            "0ee8059c419f49bcbc0f5b587339d47a",
            "846c72d5e3934461bba1cbff1526c6d6",
            "48618c580c544ef794cd4cff2b534339",
            "b00d753df8234ad083fd658d492313ff",
            "b976201342e04e3f95f32496da6c1253",
            "6ee38189f427433fb5203c06d3fb5fee",
            "46b60cf67b4648288ec6714c3c5a4404",
            "8dce3891602b4187b7f3852dca25e306",
            "c4e029be2c0b4b67826d493e8fd71fb5"
          ]
        },
        "id": "prbrphb2fkN2",
        "outputId": "d25884fd-dac1-4bea-c131-5951545d434e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "408ba7074ecd46b584e2f77767665c80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fed141bf5b284f23804ed8096af2a7b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading checkpoint /root/.cache/huggingface/metrics/bleurt/bleurt-large-512/downloads/extracted/299e33e80b83c78cc60e485384c7804f6ec1fb36c2013c5078257c17a82719ca/bleurt-large-512.\n",
            "INFO:tensorflow:Config file found, reading.\n",
            "INFO:tensorflow:Will load checkpoint bert_custom\n",
            "INFO:tensorflow:Loads full paths and checks that files exists.\n",
            "INFO:tensorflow:... name:bert_custom\n",
            "INFO:tensorflow:... vocab_file:vocab.txt\n",
            "INFO:tensorflow:... bert_config_file:bert_config.json\n",
            "INFO:tensorflow:... do_lower_case:True\n",
            "INFO:tensorflow:... max_seq_length:512\n",
            "INFO:tensorflow:Creating BLEURT scorer.\n",
            "INFO:tensorflow:Creating WordPiece tokenizer.\n",
            "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
            "INFO:tensorflow:Creating Eager Mode predictor.\n",
            "INFO:tensorflow:Loading model.\n",
            "INFO:tensorflow:BLEURT initialized.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "import nltk\n",
        "\n",
        "ROUGE = load_metric('rouge')\n",
        "BLEURT = load_metric('bleurt', 'bleurt-large-512')\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  labels = [label.strip() for label in labels]\n",
        "\n",
        "  # rougeLSum expects newline after each sentence\n",
        "  preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "  labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "  return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  preds, labels = eval_preds\n",
        "  if isinstance(preds, tuple):\n",
        "      preds = preds[0]\n",
        "\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  # Replace -100 in the labels to actual padding\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Some simple post-processing\n",
        "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "  rouge_score = ROUGE.compute(predictions=preds, references=labels)\n",
        "  rouge_score = { k: v.mid.fmeasure for k, v in rouge_score.items() }\n",
        "\n",
        "  bleurt_score = BLEURT.compute(predictions=preds, references=labels)[\"scores\"]\n",
        "  bleurt_score = {\"bleurt\": bleurt_score}\n",
        "\n",
        "  return {**rouge_score, **bleurt_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GqLy7-vl5wnq"
      },
      "outputs": [],
      "source": [
        "FINETUNE_MODEL_PATH = \"/gdrive/MyDrive/university/BigBirdModelFineTune/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Ps5DqAja7P",
        "outputId": "1ba9888d-96a4-42a0-a0c9-587faf2ea9f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ue7Jzvknuskl"
      },
      "outputs": [],
      "source": [
        "from transformers import  Seq2SeqTrainingArguments,  Seq2SeqTrainer\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "\n",
        "# setup training arguments, mainly batch size of 4\n",
        "# accumulating gradient over 2 consecutive steps and training for 3 epochs.\n",
        "# Saves the model every 900 steps and evaluates it every 300 steps.\n",
        "# Only keeps the most recent 2 models.\n",
        "# Training is made by means of gradient checkpointing and mixed-precision to \n",
        "# make the training process faster and lighter.\n",
        "training_args =  Seq2SeqTrainingArguments(\n",
        "    output_dir=FINETUNE_MODEL_PATH,\n",
        "    overwrite_output_dir=True, # used to keep training\n",
        "    gradient_accumulation_steps=2, # lower memory usage: perform backprop every 2 steps\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=250,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2, # save at most two checkpoints, delete the older ones\n",
        "    fp16=True, # faster and lighter on memory but possibly less precise on convergence\n",
        "    predict_with_generate=True,\n",
        "    gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5gL4AH3yggpT",
        "outputId": "a077308b-4f2a-4a46-8ff4-5b3e044c6578"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 17236\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 12927\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:792: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1101' max='12927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1101/12927 2:08:58 < 23:07:49, 0.14 it/s, Epoch 0.26/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>10.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.926600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.034800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.026100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-500\n",
            "Configuration saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-500/config.json\n",
            "Model weights saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-500/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:792: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "Saving model checkpoint to /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1000\n",
            "Configuration saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1000/config.json\n",
            "Model weights saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1000/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:792: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1874' max='12927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1874/12927 3:41:26 < 21:47:26, 0.14 it/s, Epoch 0.43/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>10.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.926600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.034800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.026100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.015600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1500\n",
            "Configuration saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1500/config.json\n",
            "Model weights saved in /gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [/gdrive/MyDrive/university/BigBirdModelFineTune/checkpoint-500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:792: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, \n",
        "    args=training_args,\n",
        "    #eval_dataset=dataset[\"valid\"],\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    #compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training_fp16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0878459731594640b8b5873733edbc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a2611c1e1fc4cf0b73f59a606a1ff57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b50db23543445b28dba32ae911870a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_192947130c9f40fd94e795d0be3d53b6",
            "max": 1967,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbc9d12a16524f4095c3413267f6cd1f",
            "value": 1967
          }
        },
        "0ee8059c419f49bcbc0f5b587339d47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee38189f427433fb5203c06d3fb5fee",
            "max": 1243915708,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46b60cf67b4648288ec6714c3c5a4404",
            "value": 1243915708
          }
        },
        "192947130c9f40fd94e795d0be3d53b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "408ba7074ecd46b584e2f77767665c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_631a4cd6d99a450daa9714ce06e69447",
              "IPY_MODEL_0b50db23543445b28dba32ae911870a0",
              "IPY_MODEL_bf31766f514145579aa234e9d85952a6"
            ],
            "layout": "IPY_MODEL_d82e3e29a4c849dca30aeaefb37f373d"
          }
        },
        "44d81a184216498cbc18512b911b439b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b60cf67b4648288ec6714c3c5a4404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48618c580c544ef794cd4cff2b534339": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631a4cd6d99a450daa9714ce06e69447": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d81a184216498cbc18512b911b439b",
            "placeholder": "​",
            "style": "IPY_MODEL_0a2611c1e1fc4cf0b73f59a606a1ff57",
            "value": "Downloading: "
          }
        },
        "6ee38189f427433fb5203c06d3fb5fee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "846c72d5e3934461bba1cbff1526c6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dce3891602b4187b7f3852dca25e306",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e029be2c0b4b67826d493e8fd71fb5",
            "value": " 1.24G/1.24G [00:53&lt;00:00, 54.6MB/s]"
          }
        },
        "8dce3891602b4187b7f3852dca25e306": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00d753df8234ad083fd658d492313ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b976201342e04e3f95f32496da6c1253": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbc9d12a16524f4095c3413267f6cd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf31766f514145579aa234e9d85952a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d798fc4bccb24367836be34213956acd",
            "placeholder": "​",
            "style": "IPY_MODEL_0878459731594640b8b5873733edbc2b",
            "value": " 5.19k/? [00:00&lt;00:00, 151kB/s]"
          }
        },
        "c4e029be2c0b4b67826d493e8fd71fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d798fc4bccb24367836be34213956acd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d82e3e29a4c849dca30aeaefb37f373d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb125595968344bf9232d03264b40bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00d753df8234ad083fd658d492313ff",
            "placeholder": "​",
            "style": "IPY_MODEL_b976201342e04e3f95f32496da6c1253",
            "value": "Downloading: 100%"
          }
        },
        "fed141bf5b284f23804ed8096af2a7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb125595968344bf9232d03264b40bd2",
              "IPY_MODEL_0ee8059c419f49bcbc0f5b587339d47a",
              "IPY_MODEL_846c72d5e3934461bba1cbff1526c6d6"
            ],
            "layout": "IPY_MODEL_48618c580c544ef794cd4cff2b534339"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}