{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dxgcl0g0iUw",
        "outputId": "a84c2c62-cbe4-4947-97d2-aa099940c8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 11 20:13:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gy6_XE7M2hOo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torchinfo rouge_score sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKi_g-93-nG",
        "outputId": "63519b74-cbe5-4192-cbe1-c3bec1ef412c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kUyFtE2WbINv"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgX8uQt9oCy",
        "tags": []
      },
      "source": [
        "# Dataset preparation\n",
        "\n",
        "We will tokenize the whole data by making use of the `datasets` library, which works seamlessly with the huggingface library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T4HuiUzWJ9Bw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "DATA_PATH = \"/gdrive/MyDrive/final-project/post-refactor/data/\"\n",
        "DATASET_CSV_PATH = os.path.join(DATA_PATH, \"balanced_summaries_claims_less_samples.zip\")\n",
        "\n",
        "# set maximum csv size to avoid pandas ram issues\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "# load into memory for analysis\n",
        "df = pd.read_csv(DATASET_CSV_PATH, engine=\"python\").set_index(\"patentnumber\")[[\"summary\", \"claim\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDiYSQrqf5oy"
      },
      "source": [
        "## Dataset tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk-pFOEzbKUl"
      },
      "source": [
        "Unfortunately, we won't be able to use the whole dataset. That is because BigBird-Pegasus requires a huge amount of memory in order to be finetuned.\n",
        "\n",
        "To avoid incurring into out-of-memory errors from CUDA we will only use a subset of the available dataset, by taking only those documents whose claim length is shorter than 500 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "PPlNMJltf5o1"
      },
      "outputs": [],
      "source": [
        "summary_len = df.summary.apply(lambda r: len(r.split()))\n",
        "claim_len = df.claim.apply(lambda r: len(r.split()))\n",
        "\n",
        "MAX_SUMMARY_LEN = None #@param\n",
        "MAX_CLAIM_LEN = 500 #@param\n",
        "\n",
        "subset_clause = summary_len >= 0\n",
        "if MAX_SUMMARY_LEN is not None:\n",
        "  subset_clause = subset_clause & (summary_len < MAX_SUMMARY_LEN)\n",
        "if MAX_CLAIM_LEN is not None:\n",
        "  subset_clause = subset_clause & (claim_len < MAX_CLAIM_LEN)\n",
        "\n",
        "subset_df = df[subset_clause]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3DzDN88-Dc9",
        "outputId": "035576c6-1d5c-4462-bbdc-5a11773d0797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Tokenize data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
        "TOKENIZED_DATASET_PATH = os.path.join(DATA_PATH, \"tokenized_bigbird_dataset_smaller\")\n",
        "\n",
        "# let's check if we can load the dataset from disk first.\n",
        "# this will save us the burden of loading the tokenizer\n",
        "# and tokenizing all the data we need\n",
        "if os.path.exists(TOKENIZED_DATASET_PATH):\n",
        "  dataset = load_from_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset loaded\")\n",
        "else:\n",
        "  from datasets import Dataset\n",
        "  dataset = Dataset.from_pandas(subset_df)\n",
        "\n",
        "  # first let's rename data in the way the model expect\n",
        "  dataset = dataset.rename_column(\"summary\", \"input_ids\") \\\n",
        "    .rename_column(\"claim\", \"decoder_input_ids\")\n",
        "  # even though we carefully preprocessed data some descriptions are still empty.\n",
        "  # we will filter them out\n",
        "  dataset = dataset.filter(lambda r: r[\"input_ids\"] is not None)\n",
        "\n",
        "  def encoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize the summary into input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_SUMMARY_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_SUMMARY_LEN\n",
        "\n",
        "    return tokenizer(row[\"input_ids\"], **kwargs)\n",
        "\n",
        "  # tokenize the summaries\n",
        "  dataset = dataset.map(encoder_tokenize_function, batched=True)\n",
        "\n",
        "  def decoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize claim into the expected output from the decoder \n",
        "    (decoder_input_ids and decoder_attention_mask)\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_CLAIM_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_CLAIM_LEN\n",
        "\n",
        "    tokenized = tokenizer(row[\"decoder_input_ids\"], **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"decoder_input_ids\": tokenized[\"input_ids\"],\n",
        "        \"decoder_attention_mask\": tokenized[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "  # tokenize the claim\n",
        "  dataset = dataset.map(decoder_tokenize_function, batched=True)\n",
        "\n",
        "  def compute_labels(row):\n",
        "    \"\"\"\n",
        "    Compute labels based on decoder_input_ids where padding token is represented as -100\n",
        "    \"\"\"\n",
        "    labels = row[\"decoder_input_ids\"]\n",
        "    labels = [-100 if t == 0 else t for t in labels]\n",
        "    return {\"labels\" : labels}\n",
        "  \n",
        "  dataset = dataset.map(compute_labels, batched=True)\n",
        "\n",
        "  # export the dataset to disk for future loading\n",
        "  dataset.save_to_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset computed and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYakYEpgao7F"
      },
      "source": [
        "# Dataset splits\n",
        "\n",
        "We will divide the overall dataset into the usual splits: training and testing respectively $90\\%$ and $10\\%$ of the overall data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mPyx-eCbLD9",
        "outputId": "5043daca-72a3-40c1-a0e3-746146120425"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached split indices for dataset at /gdrive/MyDrive/final-project/post-refactor/data/tokenized_bigbird_dataset_smaller/cache-22b3f2abe308373b.arrow and /gdrive/MyDrive/final-project/post-refactor/data/tokenized_bigbird_dataset_smaller/cache-2d201d7b99ae91e9.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.10, seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbS0JPhCcrE9",
        "outputId": "e2d41357-802c-4a31-d798-e006d7e5991f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 185 samples\n",
            "Test: 21 samples\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Test: {len(dataset['test'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIggHc80KgIm"
      },
      "source": [
        "# Neural model\n",
        "\n",
        "We will use Google's BigbirdPegasus (*BP*) model, by making use of huggingface APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ul_ktbMf4a",
        "outputId": "ddb9da65-aa05-4efc-a48d-708091b451b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                                                      Param #\n",
              "====================================================================================================\n",
              "BigBirdPegasusForConditionalGeneration                                      --\n",
              "├─BigBirdPegasusModel: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                       98,409,472\n",
              "│    └─BigBirdPegasusEncoder: 2-2                                           --\n",
              "│    │    └─Embedding: 3-1                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-2                   4,194,304\n",
              "│    │    └─ModuleList: 3-3                                                 201,474,048\n",
              "│    │    └─LayerNorm: 3-4                                                  2,048\n",
              "│    └─BigBirdPegasusDecoder: 2-3                                           --\n",
              "│    │    └─Embedding: 3-5                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-6                   4,194,304\n",
              "│    │    └─ModuleList: 3-7                                                 268,615,680\n",
              "│    │    └─LayerNorm: 3-8                                                  2,048\n",
              "├─Linear: 1-2                                                               98,409,472\n",
              "====================================================================================================\n",
              "Total params: 675,301,376\n",
              "Trainable params: 675,301,376\n",
              "Non-trainable params: 0\n",
              "===================================================================================================="
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration\n",
        "from torchinfo import summary\n",
        "\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
        "    \"google/bigbird-pegasus-large-bigpatent\",\n",
        "    block_size=16,\n",
        "    num_random_blocks=3,\n",
        "    attention_type=\"block_sparse\")\n",
        "model.gradient_checkpointing_enable()\n",
        "summary(model, dtypes=[\"torch.IntTensor\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOQBh4j6HfxM",
        "outputId": "4de54c90-43bd-4463-dbf0-3fc9c30c3ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ROUGE = load_metric('rouge')\n",
        "BLEU = load_metric('sacrebleu')\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  labels = [label.strip() for label in labels]\n",
        "\n",
        "  # rougeLSum expects newline after each sentence\n",
        "  preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "  labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "  return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  preds, labels = eval_preds\n",
        "  if isinstance(preds, tuple):\n",
        "      preds = preds[0]\n",
        "\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  # Replace -100 in the labels to actual padding\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Some simple post-processing\n",
        "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "  rouge_scores = ROUGE.compute(predictions=decoded_preds, \n",
        "                               references=[[dl] for dl in decoded_labels])\n",
        "  rouge_scores = { k: v.mid.fmeasure for k, v in rouge_scores.items() }\n",
        "\n",
        "  bleu_score = BLEU.compute(predictions=decoded_preds,\n",
        "                            references=[[dl] for dl in decoded_labels])[\"score\"]\n",
        "  \n",
        "  return {\"sacrebleu\": bleu_score, **rouge_scores}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GqLy7-vl5wnq"
      },
      "outputs": [],
      "source": [
        "FINETUNE_MODEL_PATH = os.path.join(DATA_PATH, \"BigBirdModelFineTuneLessSample/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ue7Jzvknuskl"
      },
      "outputs": [],
      "source": [
        "from transformers import  Seq2SeqTrainingArguments,  Seq2SeqTrainer, EarlyStoppingCallback\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 1\n",
        "ACCUMULATION_STEPS = 2\n",
        "# perform saving and evaluation every 25% of the dataset\n",
        "EVAL_LOG_STEPS = int((len(dataset['train']) / (BATCH_SIZE * ACCUMULATION_STEPS)) * 0.25)\n",
        "EVAL_ACC_STEPS = 1\n",
        "\n",
        "training_args =  Seq2SeqTrainingArguments(\n",
        "    output_dir=FINETUNE_MODEL_PATH,\n",
        "    overwrite_output_dir=True, # used to keep training\n",
        "    gradient_accumulation_steps=ACCUMULATION_STEPS, # lower memory usage: perform backprop every 2 steps\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=EVAL_LOG_STEPS,\n",
        "    eval_accumulation_steps=EVAL_ACC_STEPS,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=EVAL_LOG_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=2 * EVAL_LOG_STEPS,\n",
        "    save_total_limit=2, # save at most two checkpoints, delete the older ones\n",
        "    fp16=True, # faster and lighter on memory but possibly less precise on convergence\n",
        "    predict_with_generate=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"sacrebleu\",\n",
        "    greater_is_better=True,\n",
        "    gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gL4AH3yggpT",
        "outputId": "282aaac7-a76c-4153-839c-a4d566f41b44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, \n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xssoGlTWwvf-",
        "outputId": "6af8fa74-ec74-45cf-c9e1-099962ec4733"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 185\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 276\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='231' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [231/276 42:38 < 08:22, 0.09 it/s, Epoch 2.50/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Sacrebleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>6.708700</td>\n",
              "      <td>7.045971</td>\n",
              "      <td>25.264330</td>\n",
              "      <td>0.515152</td>\n",
              "      <td>0.417900</td>\n",
              "      <td>0.467806</td>\n",
              "      <td>0.459860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>6.624800</td>\n",
              "      <td>6.721884</td>\n",
              "      <td>39.302477</td>\n",
              "      <td>0.590481</td>\n",
              "      <td>0.482831</td>\n",
              "      <td>0.523570</td>\n",
              "      <td>0.522859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>5.570800</td>\n",
              "      <td>6.162240</td>\n",
              "      <td>42.423069</td>\n",
              "      <td>0.592859</td>\n",
              "      <td>0.469750</td>\n",
              "      <td>0.497240</td>\n",
              "      <td>0.497309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>5.152900</td>\n",
              "      <td>4.984022</td>\n",
              "      <td>41.275319</td>\n",
              "      <td>0.609829</td>\n",
              "      <td>0.488458</td>\n",
              "      <td>0.522106</td>\n",
              "      <td>0.522042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>4.364200</td>\n",
              "      <td>3.894400</td>\n",
              "      <td>36.816557</td>\n",
              "      <td>0.600010</td>\n",
              "      <td>0.499488</td>\n",
              "      <td>0.531899</td>\n",
              "      <td>0.535502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>3.677900</td>\n",
              "      <td>2.962279</td>\n",
              "      <td>36.364833</td>\n",
              "      <td>0.577800</td>\n",
              "      <td>0.467990</td>\n",
              "      <td>0.507445</td>\n",
              "      <td>0.506790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>2.755400</td>\n",
              "      <td>2.153694</td>\n",
              "      <td>39.417138</td>\n",
              "      <td>0.598344</td>\n",
              "      <td>0.470562</td>\n",
              "      <td>0.521861</td>\n",
              "      <td>0.514693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>2.107500</td>\n",
              "      <td>1.603390</td>\n",
              "      <td>39.976869</td>\n",
              "      <td>0.627739</td>\n",
              "      <td>0.510491</td>\n",
              "      <td>0.552158</td>\n",
              "      <td>0.554787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.769000</td>\n",
              "      <td>1.263733</td>\n",
              "      <td>39.172256</td>\n",
              "      <td>0.625257</td>\n",
              "      <td>0.525342</td>\n",
              "      <td>0.569315</td>\n",
              "      <td>0.565941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.471700</td>\n",
              "      <td>1.000870</td>\n",
              "      <td>38.379916</td>\n",
              "      <td>0.611600</td>\n",
              "      <td>0.497168</td>\n",
              "      <td>0.542629</td>\n",
              "      <td>0.544545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 06:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-46\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-46/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-46/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-92\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-92/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-92/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-138\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-138/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-138/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-184\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-184/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-184/pytorch_model.bin\n",
            "Deleting older checkpoint [/gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-138] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:805: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-230\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-230/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-230/pytorch_model.bin\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4f3d675227be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#trainer.evaluate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;31m# Maybe delete some older checkpoints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rotate_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_rotate_checkpoints\u001b[0;34m(self, use_mtime, output_dir)\u001b[0m\n\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m         \u001b[0;31m# Check if we should delete older checkpoint(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m         \u001b[0mcheckpoints_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sorted_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_total_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_sorted_checkpoints\u001b[0;34m(self, output_dir, checkpoint_prefix, use_mtime)\u001b[0m\n\u001b[1;32m   2215\u001b[0m         \u001b[0;31m# Make sure we don't delete the best model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m             \u001b[0mbest_model_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2218\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '/gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTuneLessSample/checkpoint-92' is not in list"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training_bigbird_balanced_dataset_small.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
