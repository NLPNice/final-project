{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dxgcl0g0iUw",
        "outputId": "8baedb12-329f-4218-dbee-855c2dc4df73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 13 07:53:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy6_XE7M2hOo"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torchinfo rouge_score sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKi_g-93-nG",
        "outputId": "1a99b810-f7e6-48c2-9bc0-0467303ea010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUyFtE2WbINv"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgX8uQt9oCy",
        "tags": []
      },
      "source": [
        "# Dataset preparation\n",
        "\n",
        "We will tokenize the whole data by making use of the `datasets` library, which works seamlessly with the huggingface library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4HuiUzWJ9Bw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "DATA_PATH = \"/gdrive/MyDrive/final-project/post-refactor/data/\"\n",
        "DATASET_CSV_PATH = os.path.join(DATA_PATH, \"summaries_claims.zip\")\n",
        "\n",
        "# set maximum csv size to avoid pandas ram issues\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "# load into memory for analysis\n",
        "df = pd.read_csv(DATASET_CSV_PATH, engine=\"python\").set_index(\"patentnumber\")[[\"summary\", \"claim\"]]\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDiYSQrqf5oy"
      },
      "source": [
        "## Dataset tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk-pFOEzbKUl"
      },
      "source": [
        "Unfortunately, we won't be able to use the whole dataset. That is because BigBird-Pegasus requires a huge amount of memory in order to be finetuned.\n",
        "\n",
        "To avoid incurring into out-of-memory errors from CUDA we will only use a subset of the available dataset, by taking only those documents whose claim length is shorter than 500 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PPlNMJltf5o1"
      },
      "outputs": [],
      "source": [
        "summary_len = df.summary.apply(lambda r: len(r.split()))\n",
        "claim_len = df.claim.apply(lambda r: len(r.split()))\n",
        "\n",
        "MAX_SUMMARY_LEN = None #@param\n",
        "MAX_CLAIM_LEN = 500 #@param\n",
        "\n",
        "subset_clause = summary_len >= 0\n",
        "if MAX_SUMMARY_LEN is not None:\n",
        "  subset_clause = subset_clause & (summary_len < MAX_SUMMARY_LEN)\n",
        "if MAX_CLAIM_LEN is not None:\n",
        "  subset_clause = subset_clause & (claim_len < MAX_CLAIM_LEN)\n",
        "\n",
        "subset_df = df[subset_clause]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3DzDN88-Dc9",
        "outputId": "2172ba8f-b84f-467a-b0ca-0a944c378587",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Tokenize data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-bigpatent\")\n",
        "TOKENIZED_DATASET_PATH = os.path.join(DATA_PATH, \"tokenized_bigbird_dataset\")\n",
        "\n",
        "# let's check if we can load the dataset from disk first.\n",
        "# this will save us the burden of loading the tokenizer\n",
        "# and tokenizing all the data we need\n",
        "if os.path.exists(TOKENIZED_DATASET_PATH):\n",
        "  dataset = load_from_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset loaded\")\n",
        "else:\n",
        "  from datasets import Dataset\n",
        "  dataset = Dataset.from_pandas(subset_df)\n",
        "\n",
        "  # first let's rename data in the way the model expect\n",
        "  dataset = dataset.rename_column(\"summary\", \"input_ids\") \\\n",
        "    .rename_column(\"claim\", \"decoder_input_ids\")\n",
        "  # even though we carefully preprocessed data some descriptions are still empty.\n",
        "  # we will filter them out\n",
        "  dataset = dataset.filter(lambda r: r[\"input_ids\"] is not None)\n",
        "\n",
        "  def encoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize the summary into input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_SUMMARY_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_SUMMARY_LEN\n",
        "\n",
        "    return tokenizer(row[\"input_ids\"], **kwargs)\n",
        "\n",
        "  # tokenize the summaries\n",
        "  dataset = dataset.map(encoder_tokenize_function, batched=True)\n",
        "\n",
        "  def decoder_tokenize_function(row):\n",
        "    \"\"\"\n",
        "    Tokenize claim into the expected output from the decoder \n",
        "    (decoder_input_ids and decoder_attention_mask)\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "    }\n",
        "\n",
        "    if MAX_CLAIM_LEN is not None:\n",
        "      kwargs[\"max_length\"] = MAX_CLAIM_LEN\n",
        "\n",
        "    tokenized = tokenizer(row[\"decoder_input_ids\"], **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"decoder_input_ids\": tokenized[\"input_ids\"],\n",
        "        \"decoder_attention_mask\": tokenized[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "  # tokenize the claim\n",
        "  dataset = dataset.map(decoder_tokenize_function, batched=True)\n",
        "\n",
        "  def compute_labels(row):\n",
        "    \"\"\"\n",
        "    Compute labels based on decoder_input_ids where padding token is represented as -100\n",
        "    \"\"\"\n",
        "    labels = row[\"decoder_input_ids\"]\n",
        "    labels = [-100 if t == 0 else t for t in labels]\n",
        "    return {\"labels\" : labels}\n",
        "  \n",
        "  dataset = dataset.map(compute_labels, batched=True)\n",
        "\n",
        "  # export the dataset to disk for future loading\n",
        "  dataset.save_to_disk(TOKENIZED_DATASET_PATH)\n",
        "  print(\"Dataset computed and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYakYEpgao7F"
      },
      "source": [
        "# Dataset splits\n",
        "\n",
        "We will divide the overall dataset into the usual splits: training and testing respectively $99.8\\%$ and $0.2\\%$ of the overall data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mPyx-eCbLD9"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.002, seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbS0JPhCcrE9",
        "outputId": "3f4eea8b-ddc1-4dc9-e396-b412009ff380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 25353 samples\n",
            "Test: 51 samples\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Test: {len(dataset['test'])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIggHc80KgIm"
      },
      "source": [
        "# Neural model\n",
        "\n",
        "We will use Google's BigbirdPegasus (*BP*) model, by making use of huggingface APIs.\n",
        "[Bigbird](https://arxiv.org/pdf/2007.14062v2.pdf) represents the BERT variant in which attention is computed.\n",
        "\n",
        "\n",
        "The model is made public with an already pretrained version on a patent dataset, [BIGPATENT](https://arxiv.org/pdf/1906.03741v1.pdf) different from the one we're using.\n",
        "The dataset consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.\n",
        "\n",
        "While general abstractive summarization is a different task than the one we're interested in, it's fairly easy to suppose that an indipendent claim could be interpreted as the summarization of the patent from a particular perspective.\n",
        "\n",
        "This model, which represents the current SOTA in the abstractive summarization field, is an incredible resource in the problem we're trying to solve.\n",
        "\n",
        "We will initially try to generate claims without any work on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ul_ktbMf4a",
        "outputId": "c88386bf-b89c-4fc9-9609-9197186502ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                                                      Param #\n",
              "====================================================================================================\n",
              "BigBirdPegasusForConditionalGeneration                                      --\n",
              "├─BigBirdPegasusModel: 1-1                                                  --\n",
              "│    └─Embedding: 2-1                                                       98,409,472\n",
              "│    └─BigBirdPegasusEncoder: 2-2                                           --\n",
              "│    │    └─Embedding: 3-1                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-2                   4,194,304\n",
              "│    │    └─ModuleList: 3-3                                                 201,474,048\n",
              "│    │    └─LayerNorm: 3-4                                                  2,048\n",
              "│    └─BigBirdPegasusDecoder: 2-3                                           --\n",
              "│    │    └─Embedding: 3-5                                                  (recursive)\n",
              "│    │    └─BigBirdPegasusLearnedPositionalEmbedding: 3-6                   4,194,304\n",
              "│    │    └─ModuleList: 3-7                                                 268,615,680\n",
              "│    │    └─LayerNorm: 3-8                                                  2,048\n",
              "├─Linear: 1-2                                                               98,409,472\n",
              "====================================================================================================\n",
              "Total params: 675,301,376\n",
              "Trainable params: 675,301,376\n",
              "Non-trainable params: 0\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration\n",
        "from torchinfo import summary\n",
        "\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
        "    \"google/bigbird-pegasus-large-bigpatent\",\n",
        "    block_size=16,\n",
        "    num_random_blocks=3,\n",
        "    attention_type=\"block_sparse\")\n",
        "model.gradient_checkpointing_enable()\n",
        "summary(model, dtypes=[\"torch.IntTensor\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ROUGE = load_metric('rouge')\n",
        "BLEU = load_metric('sacrebleu')\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  labels = [label.strip() for label in labels]\n",
        "\n",
        "  # rougeLSum expects newline after each sentence\n",
        "  preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "  labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "  return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  preds, labels = eval_preds\n",
        "  if isinstance(preds, tuple):\n",
        "      preds = preds[0]\n",
        "\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  # Replace -100 in the labels to actual padding\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Some simple post-processing\n",
        "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "  rouge_scores = ROUGE.compute(predictions=decoded_preds, \n",
        "                               references=[[dl] for dl in decoded_labels])\n",
        "  rouge_scores = { k: v.mid.fmeasure for k, v in rouge_scores.items() }\n",
        "\n",
        "  bleu_score = BLEU.compute(predictions=decoded_preds,\n",
        "                            references=[[dl] for dl in decoded_labels])[\"score\"]\n",
        "  \n",
        "  return {\"sacrebleu\": bleu_score, **rouge_scores}"
      ],
      "metadata": {
        "id": "eOQBh4j6HfxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e677b1fc-44cc-4a3d-a554-7cf5b059e91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqLy7-vl5wnq"
      },
      "outputs": [],
      "source": [
        "FINETUNE_MODEL_PATH = os.path.join(DATA_PATH, \"BigBirdModelFineTune/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue7Jzvknuskl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab01d60-5b73-4498-b481-695c1503137e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate and log every 190\n"
          ]
        }
      ],
      "source": [
        "from transformers import  Seq2SeqTrainingArguments,  Seq2SeqTrainer, EarlyStoppingCallback\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 1\n",
        "ACCUMULATION_STEPS = 2\n",
        "# perform saving and evaluation every 25% of the dataset\n",
        "EVAL_LOG_STEPS = int((len(dataset['train']) / (BATCH_SIZE * ACCUMULATION_STEPS)) * 0.015)\n",
        "EVAL_ACC_STEPS = 1\n",
        "\n",
        "print(f\"Evaluate and log every {EVAL_LOG_STEPS}\")\n",
        "\n",
        "training_args =  Seq2SeqTrainingArguments(\n",
        "    output_dir=FINETUNE_MODEL_PATH,\n",
        "    overwrite_output_dir=True, # used to keep training\n",
        "    gradient_accumulation_steps=ACCUMULATION_STEPS, # lower memory usage: perform backprop every 2 steps\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=EVAL_LOG_STEPS,\n",
        "    eval_accumulation_steps=EVAL_ACC_STEPS,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    logging_first_step=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=EVAL_LOG_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=2 * EVAL_LOG_STEPS,\n",
        "    save_total_limit=2, # save at most two checkpoints, delete the older ones\n",
        "    fp16=True, # faster and lighter on memory but possibly less precise on convergence\n",
        "    predict_with_generate=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"sacrebleu\",\n",
        "    greater_is_better=True,\n",
        "    gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gL4AH3yggpT",
        "outputId": "9b4d772b-3147-43f6-bf3b-4c0de677ffa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, \n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "xssoGlTWwvf-",
        "outputId": "240d8aa3-14a0-4ba8-c263-d6ab32ab236c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 25353\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 12676\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:809: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1521' max='12676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1521/12676 2:39:22 < 19:30:24, 0.16 it/s, Epoch 0.12/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Sacrebleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>4.304400</td>\n",
              "      <td>0.639888</td>\n",
              "      <td>38.104699</td>\n",
              "      <td>0.615534</td>\n",
              "      <td>0.491213</td>\n",
              "      <td>0.539646</td>\n",
              "      <td>0.538150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.563000</td>\n",
              "      <td>0.548252</td>\n",
              "      <td>38.496828</td>\n",
              "      <td>0.647550</td>\n",
              "      <td>0.538927</td>\n",
              "      <td>0.584246</td>\n",
              "      <td>0.582813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.520600</td>\n",
              "      <td>0.528174</td>\n",
              "      <td>42.955020</td>\n",
              "      <td>0.663484</td>\n",
              "      <td>0.550735</td>\n",
              "      <td>0.596225</td>\n",
              "      <td>0.595051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.521800</td>\n",
              "      <td>0.504398</td>\n",
              "      <td>41.268192</td>\n",
              "      <td>0.656217</td>\n",
              "      <td>0.539007</td>\n",
              "      <td>0.585333</td>\n",
              "      <td>0.586059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.488300</td>\n",
              "      <td>0.497659</td>\n",
              "      <td>42.759218</td>\n",
              "      <td>0.672829</td>\n",
              "      <td>0.556945</td>\n",
              "      <td>0.605347</td>\n",
              "      <td>0.603916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.483400</td>\n",
              "      <td>0.493202</td>\n",
              "      <td>41.148659</td>\n",
              "      <td>0.645039</td>\n",
              "      <td>0.535700</td>\n",
              "      <td>0.581814</td>\n",
              "      <td>0.581946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.505100</td>\n",
              "      <td>0.492634</td>\n",
              "      <td>41.764483</td>\n",
              "      <td>0.653928</td>\n",
              "      <td>0.545685</td>\n",
              "      <td>0.600098</td>\n",
              "      <td>0.597185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.479900</td>\n",
              "      <td>0.489531</td>\n",
              "      <td>40.340037</td>\n",
              "      <td>0.664464</td>\n",
              "      <td>0.547427</td>\n",
              "      <td>0.590520</td>\n",
              "      <td>0.589244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-380\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-380/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-380/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:809: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-760\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-760/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-760/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:809: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1140\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1140/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1140/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:809: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BigBirdPegasusForConditionalGeneration.forward` and have been ignored: patentnumber. If patentnumber are not expected by `BigBirdPegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1520\n",
            "Configuration saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1520/config.json\n",
            "Model weights saved in /gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-1520/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         )\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1627\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;31m# Maybe delete some older checkpoints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rotate_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_rotate_checkpoints\u001b[0;34m(self, use_mtime, output_dir)\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0;31m# Check if we should delete older checkpoint(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m         \u001b[0mcheckpoints_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sorted_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_total_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_sorted_checkpoints\u001b[0;34m(self, output_dir, checkpoint_prefix, use_mtime)\u001b[0m\n\u001b[1;32m   2389\u001b[0m         \u001b[0;31m# Make sure we don't delete the best model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2391\u001b[0;31m             \u001b[0mbest_model_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '/gdrive/MyDrive/final-project/post-refactor/data/BigBirdModelFineTune/checkpoint-760' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save_pretrained(os.path.join(FINETUNE_MODEL_PATH, \"final/\"))"
      ],
      "metadata": {
        "id": "MYMRsn85mFgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training_bigbird.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}